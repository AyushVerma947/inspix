# -*- coding: utf-8 -*-
"""Copy of YouTube Video Summarization with Hugging Face ASR

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11YJAQ2phpEhBjCBSAGr3xYYqT84bSdXc

# Download YouTube Video's Audio
"""


from pytube import YouTube

#VIDEO_URL = "https://www.youtube.com/watch?v=hWLf6JFbZoo" #obama

# VIDEO_URL = 'https://www.youtube.com/watch?v=H14bBuluwB8&pp=ygUGc3BlZWNo'

VIDEO_URL = 'https://www.youtube.com/watch?v=UF8uR6Z6KLc&pp=0gcJCcoJAYcqIYzv' #steve jobs 15 mins

#VIDEO_URL = 'https://youtu.be/qNJRGHk7sN8'

yt = YouTube(VIDEO_URL)

# pip install --upgrade git+https://github.com/pytube/pytube

# from pytube import YouTube
# from moviepy.editor import AudioFileClip
# import os


# # get the first audio-only stream
# stream = yt.streams.filter(only_audio=True).first()
# download_path = stream.download(filename="ytaudio.mp4")

# print("Audio downloaded:", download_path)

# # ðŸ”¹ Step 2: Convert to MP3 using moviepy
# mp3_filename = "ytaudio.mp3"

# # load mp4 audio
# audio_clip = AudioFileClip(download_path)
# audio_clip.write_audiofile(mp3_filename)
# audio_clip.close()



# !pip install -U yt-dlp

# !yt-dlp --rm-cache-dir

# !yt-dlp -f best -o "ytvideo.%(ext)s" "https://www.youtube.com/watch?v=H14bBuluwB8"

# !pip install yt-dlp --upgrade

# download best audio in m4a container (mp4-style audio)
# !yt-dlp -f "bestaudio[ext=m4a]/bestaudio" -o "ytaudio.%(ext)s" "https://www.youtube.com/watch?v=UF8uR6Z6KLc&pp=0gcJCcoJAYcqIYzv"
!yt-dlp -f "bestaudio[ext=m4a]/bestaudio" -o "ytaudio.%(ext)s" "https://www.youtube.com/watch?v=lzILoMjEpaE&pp=ygUMc2hvcnQgc3BlZWNo"

# !yt-dlp -f "bestaudio" -o "ytaudio.%(ext)s" "https://www.youtube.com/watch?v=UF8uR6Z6KLc"

# yt.streams \
#   .filter(only_audio = True, file_extension = 'mp4') \
#   .first() \
#   .download(filename = 'ytaudio')

# ! ffmpeg -i ytaudio.mp4 -acodec pcm_s16le -ar 16000 ytaudio.wav

# pip install openai-whisper

import whisper

model = whisper.load_model("base")
result = model.transcribe("ytaudio.m4a")
print(result["text"])

"""# English ASR with HuggingSound"""

# /!pip install huggingsound -q

# from huggingsound import SpeechRecognitionModel
# #

# import torch
# device = "cuda" if torch.cuda.is_available() else "cpu"

# device

# model = SpeechRecognitionModel("jonatasgrosman/wav2vec2-large-xlsr-53-english", device = device)

"""OUT OF MEMORY (OOM) error

# Audio Chunking
"""

# import librosa

# input_file = '/content/ytaudio.wav'

# print(librosa.get_samplerate(input_file))

# # Stream over 30 seconds chunks rather than load the full file
# stream = librosa.stream(
#     input_file,
#     block_length=30,
#     frame_length=16000,
#     hop_length=16000
# )

# import soundfile as sf

# for i,speech in enumerate(stream):
#   sf.write(f'{i}.wav', speech, 16000)

# i

"""# Audio Transcription / ASR / Speech to Text"""

# audio_path =[]
# for a in range(i+1):
#   audio_path.append(f'/content/{a}.wav')

# audio_path

# transcriptions = model.transcribe(audio_path)

# full_transcript = ' '

# for item in transcriptions:
#   full_transcript += ''.join(item['transcription'])

# len(full_transcript)



"""# Text Summarization"""

from transformers import pipeline

summarization = pipeline('summarization', model="Falconsai/text_summarization")

summarized_text = summarization(result["text"])

summarized_text[0]['summary_text']

from transformers import pipeline, AutoTokenizer
import re

def split_into_sentences(text):
    sentences = re.split(r'(?<=[\.\?\!])\s+', text)
    return [s.strip() for s in sentences if s.strip()]

def chunk_text_by_tokens(text, tokenizer, max_tokens=512, min_sentences_per_chunk=1):
    sentences = split_into_sentences(text)
    chunks = []
    cur = []
    cur_tokens = 0

    for sent in sentences:
        sent_tokens = len(tokenizer.encode(sent, add_special_tokens=False))
        if sent_tokens > max_tokens:
            if cur:
                chunks.append(" ".join(cur))
                cur = []
                cur_tokens = 0
            chunks.append(sent)
            continue

        if cur_tokens + sent_tokens > max_tokens and cur:
            chunks.append(" ".join(cur))
            cur = [sent]
            cur_tokens = sent_tokens
        else:
            cur.append(sent)
            cur_tokens += sent_tokens

    if cur:
        chunks.append(" ".join(cur))

    if len(chunks) > 1 and len(split_into_sentences(chunks[-1])) < min_sentences_per_chunk:
        chunks[-2] = chunks[-2] + " " + chunks[-1]
        chunks.pop()
    return chunks

def summarize_long_text(text, model_name="Falconsai/text_summarization",
                        chunk_max_tokens=None, chunk_summary_max_length=200,
                        final_summary_min_length=20, final_summary_max_length=40):
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
    summarizer = pipeline("summarization", model=model_name)

    model_max = tokenizer.model_max_length if tokenizer.model_max_length and tokenizer.model_max_length < 10**6 else 512
    if chunk_max_tokens is None:
        chunk_max_tokens = int(model_max * 0.9)

    chunks = chunk_text_by_tokens(text, tokenizer, max_tokens=chunk_max_tokens)

    chunk_summaries = []
    for i, chunk in enumerate(chunks):
        out = summarizer(chunk,
                         max_length=chunk_summary_max_length,
                         min_length=50,     # allow longer partial summaries
                         do_sample=False,
                         truncation=True)
        chunk_text = out[0]["summary_text"]
        chunk_summaries.append(chunk_text)

    combined = " ".join(chunk_summaries)
    combined_tokens = len(tokenizer.encode(combined, add_special_tokens=False))
    if combined_tokens > chunk_max_tokens:
        final_out = summarizer(combined,
                               max_length=final_summary_max_length,
                               min_length=final_summary_min_length,
                               do_sample=False,
                               truncation=True)
        final_summary = final_out[0]["summary_text"]
    else:
        final_summary = combined

    return final_summary, chunk_summaries

# Example usage:
# summary, chunk_summaries = summarize_long_text(transcript)
# print("20â€“30 word SUMMARY:\n", summary)

# summarized_text = summarization(result["text"])
full_transcript=result["text"]

summary, chunk_summaries = summarize_long_text(full_transcript, model_name="Falconsai/text_summarization")
print("FINAL SUMMARY:\n", summary)

len(summary)

summary

result["text"]



len(result["text"])

"""Text Chunking before Summarization"""



# num_iters = int(len(full_transcript)/1000)
# summarized_text = []
# for i in range(0, num_iters + 1):
#   start = 0
#   start = i * 1000
#   end = (i + 1) * 1000
#   #print("input text \n" + full_transcript[start:end])
#   out = summarization(full_transcript[start:end], min_length = 5, max_length=20)
#   out = out[0]
#   out = out['summary_text']
#  # print("Summarized text\n"+out)
#   summarized_text.append(out)

# #print(summarized_text)

